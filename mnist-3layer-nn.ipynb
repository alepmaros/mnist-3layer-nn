{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho Prático 1 - Aprendizado de Máquina\n",
    "\n",
    "<font size=4>Nome: Alexandre Maros<br></font>\n",
    "<font size=3>PPGCC - UFMG - 2018/1</font>\n",
    "\n",
    "\n",
    "Este é o _notebook_ referente ao primeiro trabalho prático da disciplina de Aprendizado de Máquina da UFMG.\n",
    "\n",
    "O objetivo é testar uma rede de 3 camadas (1 camada de entrada, 1 camada oculta e 1 camada de saída), com diferentes configurações de hiperparâmetros, para resolver o problema de reconhecer dígitos escritos a mão (como no exemplo do dataset _mnist_)\n",
    "\n",
    "A entrada da rede são 784 píxeis que corresponde a uma imagem de 28x28 referente ao dígito escrito. Cada píxel pode variar seu valor de 0 à 255, onde 0 é o píxel completamente preto e 255 o píxel branco. \n",
    "\n",
    "A saída da rede é um vetor de 10 posições, onde cada posição $i$ (0 a 9) é a probabilidade de ser o dígito $i$. Para isso utiliza-se a função de _cross-entropy_.\n",
    "\n",
    "A função de perda a ser minimizada é a seguinte\n",
    "\n",
    "![image.png](loss.jpg)\n",
    "\n",
    "Este trabalho foi desenvolvido com a ajuda da bilbioteca _scikit-learn_, _pandas_, _matplotlib_ e _numpy_.\n",
    "\n",
    "**Aviso**: A execução total desse _notebook_ é demorada!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "pd.options.display.max_rows = 10\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Leitura dos Dados\n",
    "\n",
    "Para a leitura de dados foi utilizado a biblioteca Pandas por sua facilidade em ler csv's e gerar informações dos dados caso seja necessário.\n",
    "\n",
    "Essa etapa consiste em simplesmente ler o arquivo com as 5000 entradas e separar em dois dataframes, o dataframe X, referente as features dos dados (os valores de cada pixel de cada entrada) e outro dataframe y específico para as labels das entradas (numero correto do digito de cada entrada).\n",
    "\n",
    "* O vetor X possui 5000 linhas e 784 colunas\n",
    "* O vetor y possui 5000 linhas e 1 coluna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontrar Diretorio Atual\n",
    "diretorio_atual = os.path.realpath('.')\n",
    "\n",
    "# Ler o arquivo de entrada\n",
    "dados = pd.read_csv(os.path.join(diretorio_atual, \"data_tp1\"), sep=\",\", header=None)\n",
    "\n",
    "# Armazenar o label correto dos numeros\n",
    "dados = dados.rename(columns = {0:'label'})\n",
    "y = dados.label\n",
    "\n",
    "# Retirar a primeira coluna referente aos labels\n",
    "X = dados.drop(\"label\", axis=1)\n",
    "\n",
    "# Modificar o nome das colunas\n",
    "num_imagens = X.shape[1]\n",
    "X.columns = [ int(x) for x in range(0, num_imagens)] \n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Visualização dos dados\n",
    "\n",
    "O propósito desta etapa está em analisar os dados para melhor entender como os mesmos se comportam.\n",
    "\n",
    "Inicialmente é feito a visualização das três primeiras entradas do vetor X, para verificar como é a estrutura dos dígitos e em sequência é verificado a distribuição dos dígitos dentro do dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualização dos três primeiro dígitos do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digitos = [ X.loc[0,:], X.loc[1,:], X.loc[2,:] ]\n",
    "labels = [ y[0], y[1], y[2] ]\n",
    "\n",
    "# 784 colunas correspondem a uma imagem de 28x28\n",
    "plot1 = np.reshape(digitos[0].values, (28, 28))\n",
    "plot2 = np.reshape(digitos[1].values, (28, 28))\n",
    "plot3 = np.reshape(digitos[2].values, (28, 28))\n",
    "\n",
    "fig=plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Plot the digits\n",
    "fig.add_subplot(4, 4, 1)\n",
    "plt.imshow(plot1, cmap='gray_r')\n",
    "plt.title('Label: {}'.format(labels[0]))\n",
    "\n",
    "fig.add_subplot(4, 4, 2)\n",
    "plt.imshow(plot2, cmap='gray_r')\n",
    "plt.title('Label: {}'.format(labels[1]))\n",
    "\n",
    "fig.add_subplot(4, 4, 3)\n",
    "plt.imshow(plot3, cmap='gray_r')\n",
    "plt.title('Label: {}'.format(labels[2]))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificando a distribuição dos dígitos\n",
    "\n",
    "É interessante verificar a disposição (frequência) dos dígitos no dataset. Se tiver muitos digitos 1 e poucos dígitos 9 isso pode levar a uma má previsão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y.value_counts(normalize=True).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os dígitos parecem estar bem distribuídos, sendo que os dígitos 0 e 5 são os dois digitos que menos aparecem (9.2% dos dígitos são o dígito 0 e 0.912% dos digitos são o dígito 5). O dígito 1 é o que mais aparece, com sua frequência em 11%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Construção dos Modelos\n",
    "\n",
    "Esta seção descreve a construção dos modelos e uma análise sobre seus resultados. Para gerar os modelos foi utilizado princiaplemente a classe MLPClassifier da biblioteca scikit-learn do Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Feature Scaling\n",
    "\n",
    "As redes neurais de múltiplas camandas são muito sensíveis a variância da unidade dos dados. Para diminuir esses efeitos a biblioteca scikit-learn sugere que os dados sejam \"_ajustados_\" de acordo com sua média e desvio padrão, de forma a diminuir os efeitos da variância e deixar os dados parecidos com uma distribuição normal [1]. Nesta etapa é exatamente isso que está sendo feito. Os numeros dos píxels estão sendo convertidos de [0,255] para [-1,+1]\n",
    "\n",
    "[1]: Mais informações em: http://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Definição das funções\n",
    "\n",
    "Aqui está sendo definido as funções que facilitarão a análise.\n",
    "\n",
    "A primeira função (**plot_learning_curve**) toma como principais parâmetros um modelo ainda não calculado, e as séries de treinamento.\n",
    "\n",
    "Essa função calcula o impacto que a quantidade de dados utilizados no treinamento tem no modelo e valida essa informação utilizando **10-fold cross validation**. A quantidade de treinos utilizada vai de 10% da quantidade inicial até 100%, variando de 10% a cada etapa.\n",
    "\n",
    "A função de \"pontuação\" do gráfico é a \"negative log loss\", logo, quanto maior o valor menos erro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, X, y, title=\"Default Title\", ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 10)):\n",
    "    \n",
    "    fig = plt.figure(figsize=(13, 6), dpi= 80)\n",
    "    plt.style.use('ggplot')\n",
    "\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Exemplos de Treinamento\")\n",
    "    plt.ylabel(\"Pontuação\")\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y,\n",
    "        cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring='neg_log_loss')\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Pontuação de Treinamento\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Pontuação de Cross-validation\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função **plot_loss_curve** é uma simples função para desenhar a curva de perda de cada iteração do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curve(loss_curve_, title='Default Title'):\n",
    "    fig = plt.figure(figsize=(13, 6), dpi= 80, facecolor='w', edgecolor='k')\n",
    "    plt.style.use('ggplot')\n",
    "    \n",
    "    plt.title(title)\n",
    "    \n",
    "    plt.plot(loss_curve_, '-', color='blue', linewidth=3.0)\n",
    "    \n",
    "    plt.ylabel('Função de perda')\n",
    "    plt.xlabel('Número de Iterações')\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Teste dos modelos\n",
    "\n",
    "Aqui é testado os modelos com os diferentes hiperparâmetros. São testadas as seguintes variações:\n",
    "\n",
    "1. Variação do Learning Rate\n",
    "2. Variação do Número de Perceptrons na camada oculta\n",
    "3. Variação do algoritmo de cálculo de gradiente (Gradient Descent, Stochastic Gradient Descent, Mini-Batch SGD)\n",
    "\n",
    "Os outros hiperparâmetros são fixos, sendo eles:\n",
    "\n",
    "1. Função de ativação: sigmoide\n",
    "2. Tolerância (quando o modelo para de treinar pois atingiu convergência): 1e-4\n",
    "3. Máximas iterações: 300\n",
    "4. Momentum: 0.5\n",
    "\n",
    "Cada análise de hiperparâmetro é seguido de dois gráficos, sendo o primeiro referente a curva do erro de treinamento de acordo com o número de iterações (épocas) e o segundo um gráfico de validação cruzada (usando _10-k fold_) variando a quantidade de exemplos analisados pela rede neural.\n",
    "\n",
    "No caso do segundo gráfico, o eixo _y_ é a \"pontuanção\", que é o negativo da função de perda (_neg log loss_). Quanto mais próximo do 0, mais acertos o modelo tem e consequentemente melhor ele é."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Variação do learning rate\n",
    "\n",
    "Para testar a variação do learning rate **será fixado o número de camadas na camada oculta em 50 e será usado o mini-batch SGD com 200 entradas** (esse é o _default_ do scikit-learn).\n",
    "\n",
    "Inicialmente verifica-se como o modelo se comporta quando o learning rate está configurado para **0.5, 1 e 10**, seguido de um teste com um learning rate mais baixo (**0.01**), pois é verificado que os _learning rates_ iniciais são grandes e causam _overfitting_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.5\n",
    "\n",
    "clf = MLPClassifier(solver='sgd', activation='logistic', batch_size=200, early_stopping=False,\n",
    "                    hidden_layer_sizes=(50,), learning_rate_init=learning_rate, max_iter=300, momentum=0.5,\n",
    "                    shuffle=True, verbose=False, tol=1e-4)\n",
    "clf_trained = clf.fit(X, y)\n",
    "plot_loss_curve(clf_trained.loss_curve_, title=\"Loss Curve\\nLearning Rate = {}\".format(learning_rate))\n",
    "\n",
    "\n",
    "clf = MLPClassifier(solver='sgd', activation='logistic', batch_size=200, early_stopping=False,\n",
    "                    hidden_layer_sizes=(50,), learning_rate_init=learning_rate, max_iter=300, momentum=0.5,\n",
    "                    shuffle=True, verbose=False, tol=1e-4)\n",
    "plot_learning_curve(clf, X, y, title=\"10-fold cross-validation\\nLearning Rate = {}\".format(learning_rate), n_jobs=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando o _learning rate_ é configurado para ser 0.5, o modelo converge em aproximadamente 90 iterações (épocas) e atinge um erro empírico que tende a 0).\n",
    "\n",
    "Quando realizamos a validação cruzada com diferentes quantidades de entrada, nota-se que o erro dessa validação é muito maior do que o erro de treinamento, que em todos os casos tendeu a 0. Essa situação nos mostra que possilvemente o _learning rate_ está muito elevado e o modelo está convergindo muito rápido, atingindo o estado de _overfit_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=1\n",
    "\n",
    "clf = MLPClassifier(solver='sgd', activation='logistic', batch_size=200, early_stopping=False,\n",
    "                    hidden_layer_sizes=(50,), learning_rate_init=learning_rate, max_iter=300, momentum=0.5,\n",
    "                    shuffle=True, verbose=False, tol=1e-4)\n",
    "clf_trained = clf.fit(X, y)\n",
    "plot_loss_curve(clf_trained.loss_curve_, title=\"Loss Curve\\nLearning Rate = {}\".format(learning_rate))\n",
    "\n",
    "\n",
    "clf = MLPClassifier(solver='sgd', activation='logistic', batch_size=200, early_stopping=False,\n",
    "                    hidden_layer_sizes=(50,), learning_rate_init=learning_rate, max_iter=300, momentum=0.5,\n",
    "                    shuffle=True, verbose=False, tol=1e-4)\n",
    "plot_learning_curve(clf, X, y, title=\"10-fold cross-validation\\nLearning Rate = {}\".format(learning_rate), n_jobs=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando aumentamos o erro em 0.5, a quantidade de iterações diminuiu para aproximadamente 58, significando que a convergência é ainda mais rápida. O erro também tende a 0 o que provavelmente vai indicar _overfitting_.\n",
    "\n",
    "Ao analisar a validação cruzada, chega-se a mesma conclusão anterior: O erro da validação com o erro de treinamento são grandes e o erro médio piorou (embora, como pode ser visto pelo desvio padrão, ambos são bastante semelhantes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=10\n",
    "\n",
    "clf = MLPClassifier(solver='sgd', activation='logistic', batch_size=200, early_stopping=False,\n",
    "                    hidden_layer_sizes=(50,), learning_rate_init=learning_rate, max_iter=300, momentum=0.5,\n",
    "                    shuffle=True, verbose=False, tol=1e-4)\n",
    "clf_trained = clf.fit(X, y)\n",
    "plot_loss_curve(clf_trained.loss_curve_, title=\"Loss Curve\\nLearning Rate = {}\".format(learning_rate))\n",
    "\n",
    "\n",
    "clf = MLPClassifier(solver='sgd', activation='logistic', batch_size=200, early_stopping=False,\n",
    "                    hidden_layer_sizes=(50,), learning_rate_init=learning_rate, max_iter=300, momentum=0.5,\n",
    "                    shuffle=True, verbose=False, tol=1e-4)\n",
    "plot_learning_curve(clf, X, y, title=\"10-fold cross-validation\\nLearning Rate = {}\".format(learning_rate), n_jobs=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o _learning rate_ em 10, é possível notar que a convergência é ainda mais rápida e que também, em certos pontos da curva de _loss_, a perda aumenta ao invés de diminuir de uma iteração para outra. Isso é explicado pelo alto _learning rate_, que provavelmente fez com que o modelo passasse para além do mínimo que estava seguindo.\n",
    "\n",
    "Na curva de validação, é possível notar que o modelo selecionado não é um bom modelo. Apesar da curva de treinamento tender a 0, a curva de validação nos mostra que a distância entre o erro empírico e o erro de validação é muito alta, e o erro no último caso ficou em torno de -1.5, o que é muito pior que os outros casos (fque ficaram em torno de -0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.01\n",
    "\n",
    "clf = MLPClassifier(solver='sgd', activation='logistic', batch_size=200, early_stopping=False,\n",
    "                    hidden_layer_sizes=(50,), learning_rate_init=learning_rate, max_iter=300, momentum=0.5,\n",
    "                    shuffle=True, verbose=False, tol=1e-4)\n",
    "clf_trained = clf.fit(X, y)\n",
    "plot_loss_curve(clf_trained.loss_curve_, title=\"Loss Curve\\nLearning Rate = {}\".format(learning_rate))\n",
    "\n",
    "\n",
    "clf = MLPClassifier(solver='sgd', activation='logistic', batch_size=200, early_stopping=False,\n",
    "                    hidden_layer_sizes=(50,), learning_rate_init=learning_rate, max_iter=300, momentum=0.5,\n",
    "                    shuffle=True, verbose=False, tol=1e-4)\n",
    "plot_learning_curve(clf, X, y, title=\"10-fold cross-validation\\nLearning Rate = {}\".format(learning_rate), n_jobs=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o _learning rate_ baixo (0.01), o número de iterações para convergir aumenta significativamente (em 300 ainda não convergiu completamente) e o erro não chega a 0.\n",
    "\n",
    "Entretanto, ao fazer a validação cruzada nota-se que a curva do erro de treinamento acompanha a curva do erro de validação (tem formas muito parecidas). Com o aumento de número de exemplos o erro vai diminuindo nos dois casos e dessa vez a diferença entre o erro empírico e o erro de validação estão mais próximos que os outros casos. O erro para esse _learning rate_ foi o menos dos 3 casos anteriores, tendendo a -0.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Variação do número de camadas ocultas\n",
    "\n",
    "Nesta etapa será feito a experimentação da variação da quantidade de perceptrons na camada oculta.\n",
    "\n",
    "Os tamanhos considerados para a camada oculta são: 25, 50 e 100.\n",
    "\n",
    "Será fixado o **learning_rate** em 0.01 (melhor no caso anterior) e o algoritmo de gradiente em Mini-Batch SGD (tamanho da batch de 200).\n",
    "\n",
    "Neste caso também foi aumentado o número de iterações para 1000, devido a convergência mais lenta quando o __learning_rate__ está baixo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tamanho da camada oculta = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_sizes=(25,)\n",
    "\n",
    "clf = MLPClassifier(solver='sgd', activation='logistic', batch_size=200, early_stopping=False,\n",
    "                    hidden_layer_sizes=hidden_layer_sizes, learning_rate_init=0.01, max_iter=1000, momentum=0.5,\n",
    "                    shuffle=True, verbose=False, tol=1e-4)\n",
    "clf_trained = clf.fit(X, y)\n",
    "plot_loss_curve(clf_trained.loss_curve_, title=\"Loss Curve\\nTamanho da camada oculta = {}\".format(hidden_layer_sizes))\n",
    "\n",
    "\n",
    "clf = MLPClassifier(solver='sgd', activation='logistic', batch_size=200, early_stopping=False,\n",
    "                    hidden_layer_sizes=hidden_layer_sizes, learning_rate_init=0.01, max_iter=1000, momentum=0.5,\n",
    "                    shuffle=True, verbose=False, tol=1e-4)\n",
    "plot_learning_curve(clf, X, y, title=\"10-fold cross-validation\\nTamanho da camada oculta = {}\".format(hidden_layer_sizes), n_jobs=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o quantidade de neurônios na camada oculta em 25, o modelo demora um pouco mais de 800 iterações para convergir.\n",
    "\n",
    "No gráfico de validação, o erro de cross-validation no último caso está em aproximadametne -0.37. O erro de treinamento não chegou a atingir 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tamanho da camada oculta = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hidden_layer_sizes=(50,)\n",
    "\n",
    "clf = MLPClassifier(solver='sgd', activation='logistic', batch_size=200, early_stopping=False,\n",
    "                    hidden_layer_sizes=hidden_layer_sizes, learning_rate_init=0.01, max_iter=1000, momentum=0.5,\n",
    "                    shuffle=True, verbose=False, tol=1e-4)\n",
    "clf_trained = clf.fit(X, y)\n",
    "plot_loss_curve(clf_trained.loss_curve_, title=\"Loss Curve\\nTamanho da camada oculta = {}\".format(hidden_layer_sizes))\n",
    "\n",
    "\n",
    "clf = MLPClassifier(solver='sgd', activation='logistic', batch_size=200, early_stopping=False,\n",
    "                    hidden_layer_sizes=hidden_layer_sizes, learning_rate_init=0.01, max_iter=1000, momentum=0.5,\n",
    "                    shuffle=True, verbose=False, tol=1e-4)\n",
    "plot_learning_curve(clf, X, y, title=\"10-fold cross-validation\\nTamanho da camada oculta = {}\".format(hidden_layer_sizes), n_jobs=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com a quantidade de neurônios na camada oculta em 50, o modelo convergiu um pouco mais rápido que o de com 25 camadas, em um pouco mais de 800 iterações (épocas) e não tendeu a 0, mas um pouco acima disso.\n",
    "\n",
    "No gráfico de validação cruzada, pode-se perceber que o erro da validação cruzada com o modelo de 50 perceptrons é um pouco melhor que o modelo com 25 perceptrons. O modelo com 50 perceptrons conseguiu uma potuação que se aproxima de -0.32, enquanto que o modelo com 25 perceptrons se aproximou de -0.37."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tamanho da camada oculta = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hidden_layer_sizes=(100,)\n",
    "\n",
    "clf = MLPClassifier(solver='sgd', activation='logistic', batch_size=200, early_stopping=False,\n",
    "                    hidden_layer_sizes=hidden_layer_sizes, learning_rate_init=0.01, max_iter=1000, momentum=0.5,\n",
    "                    shuffle=True, verbose=False, tol=1e-4)\n",
    "clf_trained = clf.fit(X, y)\n",
    "plot_loss_curve(clf_trained.loss_curve_, title=\"Loss Curve\\nTamanho da camada oculta = {}\".format(hidden_layer_sizes))\n",
    "\n",
    "\n",
    "clf = MLPClassifier(solver='sgd', activation='logistic', batch_size=200, early_stopping=False,\n",
    "                    hidden_layer_sizes=hidden_layer_sizes, learning_rate_init=0.01, max_iter=1000, momentum=0.5,\n",
    "                    shuffle=True, verbose=False, tol=1e-4)\n",
    "plot_learning_curve(clf, X, y, title=\"10-fold cross-validation\\nTamanho da camada oculta = {}\".format(hidden_layer_sizes), n_jobs=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O modelo com 100 perceptrons na camada oculta atingiu a convergência ainda mais rápido que os outros modelos (enquanto o modelo com 50 perceptrons atingiu a convergência com aproximadamente 820 iterações, o modelo com 100 percepetrons atingiu a convergência em 800 iterações, o que não é uma diferença tão grande).\n",
    "\n",
    "No gráfico com a validação cruzada, pode-se perceber que não houve muita diferença do erro entre o modelo com 50 perceptrons e o modelo com 100, inclusive o desvio padrão do modelo com 100 perceptrons na camada oculta apresentou um desvio padrão um pouco maior nos seus erros.\n",
    "\n",
    "Essa análise nos leva a crer que a melhor quantidade de perceptrons na camada oculta para esse problema é de 50. O modelo precisa de um pouco mais iterações para convergir mas por haver menos parâmetros para calcular, o modelo se torna um pouco mais rápido para ser computado, além de ter o menor erro dos três."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 Variação do algoritmo de gradiente e número de perceptrons na camada oculta por algoritmo\n",
    "\n",
    "Neste cenário foi considerado a variação do algoritmo de gradiente, sendo eles:\n",
    "\n",
    "1. Gradient Descent (Calcula o erro de todos os exemplos para depois ajustar o erro)\n",
    "2. Stochastic Gradient Descent (Atualiza os pesos para cada exemplo visto)\n",
    "3. Mini-Batch Stochastic Gradient Descent (Atualiza os pesos para cada $m$ pesos vistos)\n",
    "\n",
    "Além disso, aqui **também é analisado o tempo de execução de cada algoritmo**, já que os métodos possuem abordagens muito diferentes umas das outras.\n",
    "\n",
    "Para o primeiro caso (Gradient Descent), também há a análise da variação do número de perceptrons na camada oculta (para o primeiro caso, referente ao gradient descent). Não foi feito essa análise para os outros pois nos três casos os comportamentos foram muito semelhantes.\n",
    "\n",
    "**Para a análise do algoritmo Gradient Descent, também foi aumentado o número de iterações máximas para 5000**, pois o gradient descent tende a demorar muito mais para convergir.\n",
    "\n",
    "**Aviso:** A execução do algoritmo Gradient Descent é demorada. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent (Tamanho da camada oculta: 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=5000\n",
    "hidden_layer_sizes=(25,)\n",
    "\n",
    "clf = MLPClassifier(solver='sgd', activation='logistic', batch_size=batch_size, early_stopping=False,\n",
    "                    hidden_layer_sizes=hidden_layer_sizes, learning_rate_init=0.01, max_iter=5000, momentum=0.5,\n",
    "                    shuffle=True, verbose=False, tol=1e-4)\n",
    "start = time.time()\n",
    "clf_trained = clf.fit(X, y)\n",
    "end = time.time()\n",
    "print(\"Tempo de execucação (segundos):\", end - start)\n",
    "\n",
    "plot_loss_curve(clf_trained.loss_curve_, title=\"Loss Curve\\nGradient Descent (Tamanho da camada oculta: {})\".format(hidden_layer_sizes))\n",
    "\n",
    "\n",
    "clf = MLPClassifier(solver='sgd', activation='logistic', batch_size=batch_size, early_stopping=False,\n",
    "                    hidden_layer_sizes=hidden_layer_sizes, learning_rate_init=0.01, max_iter=5000, momentum=0.5,\n",
    "                    shuffle=True, verbose=False, tol=1e-4)\n",
    "plot_learning_curve(clf, X, y, title=\"10-fold cross-validation\\nGradient Descent (Tamanho da camada oculta: {})\".format(hidden_layer_sizes), n_jobs=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comentario..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent (Tamanho da camada oculta: 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=5000\n",
    "hidden_layer_sizes=(50,)\n",
    "\n",
    "clf = MLPClassifier(solver='sgd', activation='logistic', batch_size=batch_size, early_stopping=False,\n",
    "                    hidden_layer_sizes=hidden_layer_sizes, learning_rate_init=0.01, max_iter=5000, momentum=0.5,\n",
    "                    shuffle=True, verbose=False, tol=1e-4)\n",
    "\n",
    "start = time.time()\n",
    "clf_trained = clf.fit(X, y)\n",
    "end = time.time()\n",
    "print(\"Tempo de execução (segundos):\", end - start)\n",
    "\n",
    "plot_loss_curve(clf_trained.loss_curve_, title=\"Loss Curve\\nGradient Descent (Tamanho da camada oculta: {})\".format(hidden_layer_sizes))\n",
    "\n",
    "\n",
    "clf = MLPClassifier(solver='sgd', activation='logistic', batch_size=batch_size, early_stopping=False,\n",
    "                    hidden_layer_sizes=hidden_layer_sizes, learning_rate_init=0.01, max_iter=5000, momentum=0.5,\n",
    "                    shuffle=True, verbose=False, tol=1e-4)\n",
    "plot_learning_curve(clf, X, y, title=\"10-fold cross-validation\\nGradient Descent (Tamanho da camada oculta: {})\".format(hidden_layer_sizes), n_jobs=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comentario..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent (Tamanho da camada oculta: 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=5000\n",
    "hidden_layer_sizes=(100,)\n",
    "\n",
    "clf = MLPClassifier(solver='sgd', activation='logistic', batch_size=batch_size, early_stopping=False,\n",
    "                    hidden_layer_sizes=hidden_layer_sizes, learning_rate_init=0.01, max_iter=5000, momentum=0.5,\n",
    "                    shuffle=True, verbose=False, tol=1e-4)\n",
    "\n",
    "start = time.time()\n",
    "clf_trained = clf.fit(X, y)\n",
    "end = time.time()\n",
    "print(\"Tempo de execução (segundos):\", end - start)\n",
    "\n",
    "plot_loss_curve(clf_trained.loss_curve_, title=\"Loss Curve\\nGradient Descent (Tamanho da camada oculta: {})\".format(hidden_layer_sizes))\n",
    "\n",
    "\n",
    "clf = MLPClassifier(solver='sgd', activation='logistic', batch_size=batch_size, early_stopping=False,\n",
    "                    hidden_layer_sizes=hidden_layer_sizes, learning_rate_init=0.01, max_iter=5000, momentum=0.5,\n",
    "                    shuffle=True, verbose=False, tol=1e-4)\n",
    "plot_learning_curve(clf, X, y, title=\"10-fold cross-validation\\nGradient Descent (Tamanho da camada oculta: {})\".format(hidden_layer_sizes), n_jobs=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comentario..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent (Tamanho da camada oculta: 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=1\n",
    "hidden_layer_sizes=(50,)\n",
    "\n",
    "clf = MLPClassifier(solver='sgd', activation='logistic', batch_size=batch_size, early_stopping=False,\n",
    "                    hidden_layer_sizes=hidden_layer_sizes, learning_rate_init=0.01, max_iter=1000, momentum=0.5,\n",
    "                    shuffle=True, verbose=False, tol=1e-4)\n",
    "\n",
    "start = time.time()\n",
    "clf_trained = clf.fit(X, y)\n",
    "end = time.time()\n",
    "print(\"Tempo de execução (segundos):\", end - start)\n",
    "\n",
    "plot_loss_curve(clf_trained.loss_curve_, title=\"Loss Curve\\nStochastic Gradient Descent\")\n",
    "\n",
    "\n",
    "clf = MLPClassifier(solver='sgd', activation='logistic', batch_size=batch_size, early_stopping=False,\n",
    "                    hidden_layer_sizes=hidden_layer_sizes, learning_rate_init=0.01, max_iter=1000, momentum=0.5,\n",
    "                    shuffle=True, verbose=False, tol=1e-4)\n",
    "plot_learning_curve(clf, X, y, title=\"10-fold cross-validation\\nStochastic Gradient Descent\", n_jobs=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o algoritmo de Stochastic Gradient Descent, onde os pesos são atualizados a cada novo dado recebido a convergência é extremamente rápida, chegando precisar somente de 38 épocas para convergir. O erro empírico tende a 0, o que pode indicar overfitting.\n",
    "\n",
    "No gráfico da validação cruzada é possível identificar que em todos os casos o modelo chegou a atingir um erro de 0, e o erro com a maior quantidade de exemplos de treinamento ficou entre -0.32 e -0.4, um erro bom quando analisado com os outros casos.\n",
    "\n",
    "É interessante notar que, embora nesse exemplo o resultado tenha sido satisfatório, isso nem sempre acontece, já que pode atingir um mínimo local rápidamente.\n",
    "\n",
    "# ** Falar do tempo em relacao ao anterior*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini Batch SGD com 10 exemplos por batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size=10\n",
    "\n",
    "clf = MLPClassifier(solver='sgd', activation='logistic', batch_size=batch_size, early_stopping=False,\n",
    "                    hidden_layer_sizes=(50,), learning_rate_init=0.01, max_iter=500, momentum=0.5,\n",
    "                    shuffle=True, verbose=False, tol=1e-4)\n",
    "\n",
    "start = time.time()\n",
    "clf_trained = clf.fit(X, y)\n",
    "end = time.time()\n",
    "print(\"Tempo de execução (segundos):\", end - start)\n",
    "\n",
    "plot_loss_curve(clf_trained.loss_curve_, title=\"Loss Curve\\nMini-Batch Stochastic Gradient Descent ({})\".format(batch_size))\n",
    "\n",
    "\n",
    "clf = MLPClassifier(solver='sgd', activation='logistic', batch_size=batch_size, early_stopping=False,\n",
    "                    hidden_layer_sizes=(50,), learning_rate_init=0.01, max_iter=500, momentum=0.5,\n",
    "                    shuffle=True, verbose=False, tol=1e-4)\n",
    "plot_learning_curve(clf, X, y, title=\"10-fold cross-validation\\nMini-Batch Stochastic Gradient Descent ({})\".format(batch_size), n_jobs=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o uso do Mini-batch SGD com 10 exemplos (os pesos são analisados após o cálculo dos erros de 10 exemplos), a convergência demora um pouco mais e também tende a 0.\n",
    "\n",
    "Na validação cruzada, dá para se notar que em todos os casos o erro tendeu a 0 (embora não tenha chegado a 0) e a pontuação da validação cruzada ficou muito parecida quando utilizado o modelo com o algoritmo Stochastic Gradient Descent.\n",
    "\n",
    "Quanto a análise do tempo, o modelo foi um pouco mais rápidio que o Stochastic Gradient Descent, levando 52 segundos para calcular o modelo com 5000 exemplos (Embora o SGD tenha convergido em menos iterações, nesse exemplo há menos cálculos de atualização de peso). Pelo erro ser muito parecido com o SGD, esse modelo se torna mais interessante de ser utilizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini Batch SGD com 50 exemplos por batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=50\n",
    "hidden_layer_sizes=(50,)\n",
    "\n",
    "clf = MLPClassifier(solver='sgd', activation='logistic', batch_size=batch_size, early_stopping=False,\n",
    "                    hidden_layer_sizes=hidden_layer_sizes, learning_rate_init=0.01, max_iter=500, momentum=0.5,\n",
    "                    shuffle=True, verbose=False, tol=1e-4)\n",
    "\n",
    "start = time.time()\n",
    "clf_trained = clf.fit(X, y)\n",
    "end = time.time()\n",
    "print(\"Tempo de execução (segundos):\", end - start)\n",
    "\n",
    "plot_loss_curve(clf_trained.loss_curve_, title=\"Loss Curve\\nMini-Batch Stochastic Gradient Descent ({})\".format(batch_size))\n",
    "\n",
    "\n",
    "clf = MLPClassifier(solver='sgd', activation='logistic', batch_size=batch_size, early_stopping=False,\n",
    "                    hidden_layer_sizes=hidden_layer_sizes, learning_rate_init=0.01, max_iter=500, momentum=0.5,\n",
    "                    shuffle=True, verbose=False, tol=1e-4)\n",
    "plot_learning_curve(clf, X, y, title=\"10-fold cross-validation\\nMini-Batch Stochastic Gradient Descent ({})\\n\".format(batch_size), n_jobs=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Mini-Batch SGD com 50 exemplos precisou de ainda mais iterações para convergiu e foi o que menos teve overfit em seus dados de treinamento em relação ao SGD e o outro Mini-Batch SGD (teve o maior valor na função de perda dos dois algoritmos).\n",
    "\n",
    "O erro da validação cruzada também foi o melhor dos outros algoritmos, se aproximando de -0.34, com um desvio padrão também menor que os outros.\n",
    "\n",
    "O tempo de execução também foi o menor dos algoritmos, com 38 segundos para calcular o modelo com todos os 5000 exemplos. Novamente, o número de iterações não está diretamente relacionado ao tempo, já que nesse caso há mais exemplos sendo analisados até atualizar os pesos das entradas dos perceptrons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusões\n",
    "\n",
    "Neste trabalho foi visto..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
